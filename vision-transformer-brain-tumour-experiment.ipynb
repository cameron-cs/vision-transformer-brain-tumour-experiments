{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDMEaIikkZ-b",
    "outputId": "588f15d6-1ac5-43d3-8abd-31c2eaf9af16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FF8J1Na8lkiT"
   },
   "outputs": [],
   "source": [
    "mkdir callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sTUHgW4p7CB"
   },
   "outputs": [],
   "source": [
    "train_dataset = BrainTumorDataset(root_dir='/content/drive/MyDrive/tumour_data_archive/Training', transform=transform)\n",
    "test_dataset = BrainTumorDataset(root_dir='/content/drive/MyDrive/tumour_data_archive/Testing', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SS1Tm3m-lqUz",
    "outputId": "210f9f84-2925-44ca-b3fb-056f37bd291f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "FOLD 1\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:39<00:00,  3.64it/s, accuracy=51.5, loss=0.834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8790, Accuracy: 51.54%\n",
      "Validation Loss: 0.1573, Validation Accuracy: 69.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.60it/s, accuracy=71.7, loss=0.974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.5826, Accuracy: 71.72%\n",
      "Validation Loss: 0.1321, Validation Accuracy: 72.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:33<00:00,  4.26it/s, accuracy=76.5, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.4850, Accuracy: 76.47%\n",
      "Validation Loss: 0.1080, Validation Accuracy: 79.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.50it/s, accuracy=78.5, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.4376, Accuracy: 78.51%\n",
      "Validation Loss: 0.1018, Validation Accuracy: 80.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.43it/s, accuracy=81.4, loss=0.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.3925, Accuracy: 81.40%\n",
      "Validation Loss: 0.0932, Validation Accuracy: 82.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.49it/s, accuracy=82.1, loss=0.627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.3666, Accuracy: 82.14%\n",
      "Validation Loss: 0.0857, Validation Accuracy: 83.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.50it/s, accuracy=84.3, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.3292, Accuracy: 84.29%\n",
      "Validation Loss: 0.0789, Validation Accuracy: 85.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.52it/s, accuracy=84.9, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.3197, Accuracy: 84.90%\n",
      "Validation Loss: 0.0796, Validation Accuracy: 85.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.53it/s, accuracy=85.4, loss=0.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.3067, Accuracy: 85.45%\n",
      "Validation Loss: 0.0766, Validation Accuracy: 85.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.52it/s, accuracy=85.8, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.2926, Accuracy: 85.75%\n",
      "Validation Loss: 0.0770, Validation Accuracy: 85.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.57it/s, accuracy=85.5, loss=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.2914, Accuracy: 85.51%\n",
      "Validation Loss: 0.0774, Validation Accuracy: 84.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.53it/s, accuracy=86.6, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 0.2868, Accuracy: 86.56%\n",
      "Validation Loss: 0.0794, Validation Accuracy: 85.83%\n",
      "Early stopping triggered\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:30<00:00,  4.62it/s, accuracy=56.7, loss=0.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8121, Accuracy: 56.73%\n",
      "Validation Loss: 0.1668, Validation Accuracy: 67.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.48it/s, accuracy=73.8, loss=0.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.5516, Accuracy: 73.80%\n",
      "Validation Loss: 0.1408, Validation Accuracy: 71.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.56it/s, accuracy=76.8, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.4776, Accuracy: 76.80%\n",
      "Validation Loss: 0.1258, Validation Accuracy: 74.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.56it/s, accuracy=78.8, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.4351, Accuracy: 78.84%\n",
      "Validation Loss: 0.1202, Validation Accuracy: 76.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.60it/s, accuracy=80.5, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.4013, Accuracy: 80.54%\n",
      "Validation Loss: 0.1133, Validation Accuracy: 77.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.53it/s, accuracy=82.5, loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.3626, Accuracy: 82.47%\n",
      "Validation Loss: 0.0987, Validation Accuracy: 81.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.36it/s, accuracy=83.4, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.3431, Accuracy: 83.39%\n",
      "Validation Loss: 0.0921, Validation Accuracy: 82.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:30<00:00,  4.65it/s, accuracy=84, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.3333, Accuracy: 83.96%\n",
      "Validation Loss: 0.0851, Validation Accuracy: 83.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:30<00:00,  4.62it/s, accuracy=85.3, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.3015, Accuracy: 85.27%\n",
      "Validation Loss: 0.0913, Validation Accuracy: 83.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.50it/s, accuracy=85.3, loss=0.355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.3049, Accuracy: 85.29%\n",
      "Validation Loss: 0.0869, Validation Accuracy: 83.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.57it/s, accuracy=85.7, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.2963, Accuracy: 85.71%\n",
      "Validation Loss: 0.0862, Validation Accuracy: 83.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.45it/s, accuracy=85.5, loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 0.2967, Accuracy: 85.47%\n",
      "Validation Loss: 0.0846, Validation Accuracy: 83.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.57it/s, accuracy=85.7, loss=0.463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Loss: 0.2982, Accuracy: 85.66%\n",
      "Validation Loss: 0.0842, Validation Accuracy: 83.29%\n",
      "Early stopping triggered\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.45it/s, accuracy=54.6, loss=0.645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8433, Accuracy: 54.60%\n",
      "Validation Loss: 0.1584, Validation Accuracy: 68.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.60it/s, accuracy=71.1, loss=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.5808, Accuracy: 71.05%\n",
      "Validation Loss: 0.1287, Validation Accuracy: 75.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.61it/s, accuracy=74.9, loss=0.891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.5040, Accuracy: 74.92%\n",
      "Validation Loss: 0.1288, Validation Accuracy: 76.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.38it/s, accuracy=77.4, loss=0.653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.4550, Accuracy: 77.37%\n",
      "Validation Loss: 0.1047, Validation Accuracy: 81.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.57it/s, accuracy=80, loss=0.864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.4113, Accuracy: 80.02%\n",
      "Validation Loss: 0.1042, Validation Accuracy: 81.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.61it/s, accuracy=81.9, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.3825, Accuracy: 81.95%\n",
      "Validation Loss: 0.1065, Validation Accuracy: 79.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.55it/s, accuracy=83.1, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.3483, Accuracy: 83.09%\n",
      "Validation Loss: 0.0872, Validation Accuracy: 83.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.34it/s, accuracy=83.8, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.3349, Accuracy: 83.76%\n",
      "Validation Loss: 0.0806, Validation Accuracy: 85.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.37it/s, accuracy=84.4, loss=0.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.3216, Accuracy: 84.42%\n",
      "Validation Loss: 0.0846, Validation Accuracy: 84.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.59it/s, accuracy=85.1, loss=0.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.3052, Accuracy: 85.05%\n",
      "Validation Loss: 0.0809, Validation Accuracy: 85.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.45it/s, accuracy=85.6, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.3048, Accuracy: 85.60%\n",
      "Validation Loss: 0.0811, Validation Accuracy: 84.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.48it/s, accuracy=85.5, loss=0.492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 0.2994, Accuracy: 85.54%\n",
      "Validation Loss: 0.0806, Validation Accuracy: 85.99%\n",
      "Early stopping triggered\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.39it/s, accuracy=56.3, loss=0.904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8240, Accuracy: 56.26%\n",
      "Validation Loss: 0.1531, Validation Accuracy: 70.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:33<00:00,  4.27it/s, accuracy=71.1, loss=0.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.5709, Accuracy: 71.07%\n",
      "Validation Loss: 0.1467, Validation Accuracy: 72.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.34it/s, accuracy=75.8, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.4930, Accuracy: 75.75%\n",
      "Validation Loss: 0.1161, Validation Accuracy: 78.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.45it/s, accuracy=78.9, loss=0.658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.4376, Accuracy: 78.86%\n",
      "Validation Loss: 0.1092, Validation Accuracy: 78.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.46it/s, accuracy=80.5, loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.3980, Accuracy: 80.48%\n",
      "Validation Loss: 0.1008, Validation Accuracy: 79.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.43it/s, accuracy=82.6, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.3585, Accuracy: 82.58%\n",
      "Validation Loss: 0.0923, Validation Accuracy: 81.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:33<00:00,  4.27it/s, accuracy=83.5, loss=0.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.3393, Accuracy: 83.46%\n",
      "Validation Loss: 0.0876, Validation Accuracy: 84.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.37it/s, accuracy=84.5, loss=0.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.3231, Accuracy: 84.51%\n",
      "Validation Loss: 0.0885, Validation Accuracy: 84.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:33<00:00,  4.33it/s, accuracy=85.8, loss=0.535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.3000, Accuracy: 85.80%\n",
      "Validation Loss: 0.0886, Validation Accuracy: 83.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.52it/s, accuracy=85.5, loss=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.2987, Accuracy: 85.54%\n",
      "Validation Loss: 0.0800, Validation Accuracy: 83.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.42it/s, accuracy=85.6, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.3027, Accuracy: 85.56%\n",
      "Validation Loss: 0.0813, Validation Accuracy: 84.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:34<00:00,  4.16it/s, accuracy=85, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 0.3014, Accuracy: 85.01%\n",
      "Validation Loss: 0.0823, Validation Accuracy: 83.71%\n",
      "Early stopping triggered\n",
      "--------------------------------\n",
      "FOLD 5\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.41it/s, accuracy=57.9, loss=0.802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8129, Accuracy: 57.88%\n",
      "Validation Loss: 0.1424, Validation Accuracy: 71.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.55it/s, accuracy=72.7, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.5593, Accuracy: 72.74%\n",
      "Validation Loss: 0.1255, Validation Accuracy: 74.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.58it/s, accuracy=76.7, loss=0.444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.4763, Accuracy: 76.67%\n",
      "Validation Loss: 0.1021, Validation Accuracy: 78.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.38it/s, accuracy=79.2, loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.4381, Accuracy: 79.21%\n",
      "Validation Loss: 0.0981, Validation Accuracy: 80.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.50it/s, accuracy=81.3, loss=0.481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.3905, Accuracy: 81.29%\n",
      "Validation Loss: 0.0891, Validation Accuracy: 82.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:30<00:00,  4.62it/s, accuracy=82.1, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.3711, Accuracy: 82.06%\n",
      "Validation Loss: 0.0910, Validation Accuracy: 81.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.50it/s, accuracy=83.2, loss=0.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.3477, Accuracy: 83.22%\n",
      "Validation Loss: 0.0778, Validation Accuracy: 84.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.57it/s, accuracy=84.5, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.3231, Accuracy: 84.51%\n",
      "Validation Loss: 0.0757, Validation Accuracy: 85.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.34it/s, accuracy=85.3, loss=0.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.3116, Accuracy: 85.32%\n",
      "Validation Loss: 0.0764, Validation Accuracy: 85.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.42it/s, accuracy=84.8, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.3149, Accuracy: 84.79%\n",
      "Validation Loss: 0.0740, Validation Accuracy: 86.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:31<00:00,  4.55it/s, accuracy=86.5, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.2974, Accuracy: 86.48%\n",
      "Validation Loss: 0.0735, Validation Accuracy: 85.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:32<00:00,  4.44it/s, accuracy=85.7, loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 0.3020, Accuracy: 85.71%\n",
      "Validation Loss: 0.0712, Validation Accuracy: 85.55%\n",
      "Early stopping triggered\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:07<00:00,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# custom dataset class\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        for label, class_dir in enumerate(self.classes):\n",
    "            class_dir_path = os.path.join(root_dir, class_dir)\n",
    "            for image_name in os.listdir(class_dir_path):\n",
    "                self.image_paths.append(os.path.join(class_dir_path, image_name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Transformations for the images\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(10),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "root_dir = '/content/drive/MyDrive/tumour_data_archive/Training'\n",
    "dataset = BrainTumorDataset(root_dir=root_dir, transform=transform)\n",
    "\n",
    "# define Vision Transformer model\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, grid_size, grid_size)\n",
    "        x = x.flatten(2)  # (B, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q = self.query(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.key(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.value(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, num_classes=4, embed_dim=128, depth=4, num_heads=4, mlp_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans=1, embed_dim=embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "        for _ in range(depth)])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]\n",
    "        x = self.head(cls_token_final)\n",
    "        return x\n",
    "\n",
    "# Cross-validation setup\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'FOLD {fold+1}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=32, sampler=val_subsampler)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = VisionTransformer(num_classes=4).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{epochs}]')\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        scheduler.step()\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n",
    "\n",
    "        if early_stopping(val_loss):\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Final evaluation on test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "progress_bar = tqdm(test_loader, desc='Testing')\n",
    "with torch.no_grad():\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'callback/vision_transformer_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDo4XoZxg5qd"
   },
   "source": [
    "# Vision Transformer for Brain Tumor classification (experiments)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Brain tumors are abnormal growths of cells in the brain, which can be either benign (non-cancerous) or malignant (cancerous). Early detection and classification of brain tumors are crucial for effective treatment and improving patient outcomes.\n",
    "\n",
    "The implementation of a Vision Transformer (ViT) model for the classification of brain tumors using MRI images. The model is trained and evaluated using a dataset of brain MRI images, and the performance is analyzed using K-fold cross-validation.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset consists of MRI images of brain tumors classified into four categories:\n",
    "- Pituitary\n",
    "- No tumor\n",
    "- Meningioma\n",
    "- Glioma\n",
    "\n",
    "\n",
    "### Training\n",
    "- pituitary/\n",
    "- notumor/\n",
    "- meningioma/\n",
    "- glioma/\n",
    "\n",
    "### Testing\n",
    "- pituitary/\n",
    "- notumor/\n",
    "- meningioma/\n",
    "- glioma/\n",
    "\n",
    "\n",
    "Each directory contains images of brain MRIs corresponding to the respective class.\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "### Transformations\n",
    "\n",
    "The following transformations are applied to the images:\n",
    "- Resize the images to 224 x 224 pixels.\n",
    "- Random horizontal flip.\n",
    "- Random rotation of 10 degrees.\n",
    "- Convert the images to tensors.\n",
    "- Normalize the images with a mean and standard deviation of 0.5.\n",
    "\n",
    "### Custom Dataset class\n",
    "\n",
    "A custom dataset class is used to load the images and their corresponding labels from the directory structure. This class handles the loading of image paths, converting images to grayscale, and applying the transformations.\n",
    "\n",
    "## Vision Transformer model\n",
    "\n",
    "### Patch Embedding\n",
    "\n",
    "The Patch Embedding layer splits the input image into patches and projects each patch into a high-dimensional space using a convolutional layer. The image of size $$( H \\times W \\times C)$$ is divided into patches of size $$( P \\times P )$$. Each patch is then linearly transformed into a vector of size \\( D \\).\n",
    "\n",
    "The number of patches N is given by:\n",
    "\n",
    "$$\n",
    "[ N = \\left(\\frac{H}{P}\\right) \\times \\left(\\frac{W}{P}\\right)]\n",
    "$$\n",
    "\n",
    "The patches are then flattened and transposed to form a sequence of patches.\n",
    "\n",
    "### Multi-Head Self-Attention\n",
    "\n",
    "The Multi-Head Self-Attention layer allows the model to focus on different parts of the input sequence (patches) simultaneously. It computes the attention weights and applies them to the value vectors. The attention mechanism is defined as:\n",
    "\n",
    "$$\n",
    "[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V ]\n",
    "$$\n",
    "\n",
    "where Q, K, and V are the query, key, and value matrices, respectively, and \\( d_k \\) is the dimension of the key vectors. The attention weights are computed as the dot product of the query and key matrices, scaled by sqrt(ð‘‘ð‘˜), and then passed through a softmax function.\n",
    "\n",
    "### Transformer Block\n",
    "\n",
    "The Transformer Block consists of a multi-head self-attention layer followed by a feed-forward neural network. Both layers are followed by layer normalization and residual connections. The feed-forward network is defined as:\n",
    "\n",
    "$$\n",
    "[ \\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2]\n",
    "$$\n",
    "\n",
    "where ð‘Š1, ð‘Š2, ð‘1, and ð‘2 are learnable parameters, and GELU is the Gaussian Error Linear Unit activation function. The residual connections are added to stabilize training:\n",
    "\n",
    "$$\n",
    "[ x' = x + \\text{SelfAttention}(x)]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "[ x'' = x' + \\text{FFN}(x')]\n",
    "$$\n",
    "\n",
    "### Vision Transformer\n",
    "\n",
    "The Vision Transformer model consists of a patch embedding layer, multiple transformer blocks, and a final classification head. The input image is first divided into patches and embedded into a higher-dimensional space. A class token is added to the sequence of patches, and positional embeddings are added to retain spatial information. The sequence is then passed through multiple transformer blocks, and the final class token is used for classification.\n",
    "\n",
    "## Training and Evaluation\n",
    "\n",
    "### Cross-Validation Setup\n",
    "\n",
    "K-fold cross-validation is used to evaluate the model. The dataset is split into \\( k \\) folds, and the model is trained and validated on different subsets of the data. In this setup, 5-fold cross-validation is used, where the dataset is divided into 5 subsets. In each iteration, one subset is used for validation, and the remaining four are used for training.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "The training loop includes forward and backward passes, loss computation, and optimization steps. The learning rate is adjusted using a cosine annealing scheduler, and early stopping is implemented to prevent overfitting. The loss function used is cross-entropy loss, which is suitable for multi-class classification tasks.\n",
    "\n",
    "The optimization process involves minimizing the cross-entropy loss:\n",
    "\n",
    "$$\n",
    "[ \\text{CrossEntropyLoss} = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c)]\n",
    "$$\n",
    "\n",
    "where C is the number of classes, ð‘¦ð‘ is the true label, and ð‘¦Ì‚ð‘ is the predicted probability for class c.\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Early stopping is used to prevent overfitting by monitoring the validation loss. If the validation loss does not improve for a certain number of epochs (patience), training is stopped.\n",
    "\n",
    "### Results\n",
    "\n",
    "The training process produces the following results:\n",
    "\n",
    "- **FOLD 1**\n",
    "    - Epoch 1: Loss: 0.8790, Accuracy: 51.54%, Validation Loss: 0.1573, Validation Accuracy: 69.47%\n",
    "    - Epoch 2: Loss: 0.5826, Accuracy: 71.72%, Validation Loss: 0.1321, Validation Accuracy: 72.70%\n",
    "    - ...\n",
    "    - Epoch 12: Loss: 0.2868, Accuracy: 86.56%, Validation Loss: 0.0794, Validation Accuracy: 85.83%\n",
    "    - Early stopping triggered\n",
    "\n",
    "- **FOLD 2**\n",
    "    - Epoch 1: Loss: 0.8121, Accuracy: 56.73%, Validation Loss: 0.1668, Validation Accuracy: 67.54%\n",
    "    - Epoch 2: Loss: 0.5516, Accuracy: 73.80%, Validation Loss: 0.1408, Validation Accuracy: 71.65%\n",
    "    - ...\n",
    "    - Epoch 13: Loss: 0.2982, Accuracy: 85.66%, Validation Loss: 0.0842, Validation Accuracy: 83.29%\n",
    "    - Early stopping triggered\n",
    "\n",
    "- **FOLD 3**\n",
    "    - Epoch 1: Loss: 0.8433, Accuracy: 54.60%, Validation Loss: 0.1584, Validation Accuracy: 68.56%\n",
    "    - Epoch 2: Loss: 0.5808, Accuracy: 71.05%, Validation Loss: 0.1287, Validation Accuracy: 75.57%\n",
    "    - ...\n",
    "    - Epoch 12: Loss: 0.2994, Accuracy: 85.54%, Validation Loss: 0.0806, Validation Accuracy: 85.99%\n",
    "    - Early stopping triggered\n",
    "\n",
    "- **FOLD 4**\n",
    "    - Epoch 1: Loss: 0.8240, Accuracy: 56.26%, Validation Loss: 0.1531, Validation Accuracy: 70.67%\n",
    "    - Epoch 2: Loss: 0.5709, Accuracy: 71.07%, Validation Loss: 0.1467, Validation Accuracy: 72.24%\n",
    "    - ...\n",
    "    - Epoch 12: Loss: 0.3014, Accuracy: 85.01%, Validation Loss: 0.0823, Validation Accuracy: 83.71%\n",
    "    - Early stopping triggered\n",
    "\n",
    "- **FOLD 5**\n",
    "    - Epoch 1: Loss: 0.8129, Accuracy: 57.88%, Validation Loss: 0.1424, Validation Accuracy: 71.89%\n",
    "    - Epoch 2: Loss: 0.5593, Accuracy: 72.74%, Validation Loss: 0.1255, Validation Accuracy: 74.69%\n",
    "    - ...\n",
    "    - Epoch 12: Loss: 0.3020, Accuracy: 85.71%, Validation Loss: 0.0712, Validation Accuracy: 85.55%\n",
    "    - Early stopping triggered\n",
    "\n",
    "- **Testing**: Test Accuracy: 84.06%\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Vision Transformer model for brain tumor classification using MRI images demonstrates promising performance with an overall test accuracy of 84.06%. The use of data augmentation, cross-validation, and early stopping contributes to the model's effectiveness in classifying brain tumors. Further improvements can be made by fine-tuning hyperparameters, increasing the dataset size, and exploring more advanced architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CuEsJs8qGHV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 1
}
